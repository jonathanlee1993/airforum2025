---
title: "Flexing Data: How Campus Gym Engagement Boosts Student Performance"
author: "Jonathan Lee"
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    code-fold: true
    code-summary: "Show Code"
    number-sections: false
editor: visual
---

## Project Overview

This project investigates the relationships between student usage of the University of West Florida (UWF) Health Leisure and Sports (HLS) Facility recreation center, group fitness activities, and intramural sports participation with student performance indicators such as full-time first time in college (FTIC) four year graduation rate and full-time FTIC academic progress rate (defined as a student being enrolled in the second fall with a cumulative GPA of 2.0 or higher) using logistic regression. The goal is to identify the strength of any existing relationships between those students performance indicators with any HLS usage data, after taking a variety of student demographic and performance factors into account.

To improve model performance and handle potential class imbalance, we apply hyperparameter tuning using the \`tidymodels\` framework and evaluate model performance with appropriate metrics.

This guide is written as a reproducible Quarto document for sharing, documentation, and educational purposes.

## Setup

We'll use \`tidymodels\` as the modeling framework and include additional packages for cleaning, preprocessing, and visualization. The following code block gives a comprehensive list of packages that you will need to load (or install if you have not installed them):

```{r}
#| echo: false
#| message: false
#| warning: false
library(uwfir)
init_jlee1_env_keyring()
```

```{r}
#| message: false
#| warning: false
# Optional: install these if not already installed
# install.packages(
#   c(
#     "tidymodels", "themis", "janitor", "skimr", "vip", "correlationfunnel",
#     "tidyverse", "ggplot", "plotly", "DT", "shiny", "shinyWidgets", "shinyjs"
#   )
# )
# 
# Install my custom package from GitHub
# This will give you access to a few custom functions I wrote for this analysis
# devtools::install_github("")

# LOAD LIBRARIES

# Custom package
# library()

# Tidyverse
library(tidyverse)         # Core data science packages for data manipulation, visualization, and piping
library(tidymodels)        # Framework for modeling and machine learning using the tidyverse philosophy

# Modeling (most loaded via tidyverse or tidymodels, but explicitly listed here for clarity)
library(broom)             # Converts model objects into tidy tibbles
library(parsnip)           # Unified interface to create and fit models
library(themis)            # Tools for dealing with class imbalance (e.g., SMOTE, downsampling)
library(rsample)           # Functions for resampling (e.g., train/test splits, cross-validation)    
library(recipes)           # Preprocessing steps like normalization, encoding, feature engineering    
library(workflows)         # Combines preprocessing and modeling into a single workflow
library(workflowsets)      # Enables tuning across multiple workflows simultaneously
library(dials)             # Defines hyperparameters for tuning
library(tune)              # Tools for hyperparameter tuning and grid search
library(yardstick)         # Metrics for evaluating model performance (accuracy, ROC AUC, etc.)

# Plots
library(vip)               # For generating variable importance plots for ML models
library(ggplot2)           # Grammar of graphics system used to create static plots
library(plotly)            # Makes ggplot2 graphs interactive and supports dynamic web visualizations
library(tidyquant)         # Combines tidyverse with financial and time series analysis tools, but used here for plotting purposes

# Shiny
library(shiny)             # Web application framework for R
library(shinyWidgets)      # Custom UI widgets to enhance Shiny app interactivity and design
library(shinyjs)           # Enables use of JavaScript functions in Shiny apps (e.g., show/hide elements)

# Plotting Elements
library(DT)                # Creates interactive data tables in Shiny or R Markdown
library(correlationfunnel) # Helps visualize and explore correlations between features and binary outcomes
```

## Load Data

We begin by loading the required datasets. For portability, we assume the working directory is the root of the project folder, and all data files are stored in the \`./data/\` directory.

```{r}
#| message: false
# * Student data: FT FTIC Cohort students with outcome and covariate data included
student_data_tbl <- readr::read_rds("./data/student_data_tbl.rds") %>% 
  filter(between(DEMO_TIME_FRAME, 201808, 202208))

# * Rec Center Swipe data
user_swipe_tbl <- readr::read_rds("./data/user_swipe_tbl.rds")

# * Intramurals data
intramural_data_tbl <- readr::read_rds("./data/intramural_data_tbl.rds")

# * Group Fitness data
group_fitness_data_tbl <- readr::read_rds("./data/group_fitness_data_tbl.rds")
```

## Clean Data

We perform a few steps to clean the gym swipe, intramural, and group fitness data so that they are easier to use and easier to join to the student data, and to select the relevant gym swipe data/filter irrelevant data (such as blocked entries):

```{r}
#| message: false
# * Intramural: Winter Year should have vqkterm_code manually assigned to (File_Year+1)01
# * vqkterm_code is a field associated with an academic term: for example, 202401 = spring 2024
intramural_data_tbl <- intramural_data_tbl %>% 
  mutate(
    VQKTERM_CODE = if_else(is.na(VQKTERM_CODE), as.character(as.integer(str_glue("{File_Year}08"))+93), VQKTERM_CODE)
  ) %>% 
  # ** Change Student ID and Term Code types to Integer
  mutate(
    across(c(UWF.ID.., VQKTERM_CODE), as.integer)
  )

# Group Fitness: Change Student ID and Year types to Integer, and convert Year to Fall Term [File_Year]08
group_fitness_data_tbl <- group_fitness_data_tbl %>% 
  mutate(
    across(c(File_Year, Student.ID), as.integer),
    across(File_Year, ~100*.x + 8)
  )

# * Rec Center Swipes: Remove blocked entries, get entry/exit/center data ----
user_swipe_tbl <- user_swipe_tbl %>% 
  filter(
    blocked == 0,
    str_detect(source, "Entr|Exit|-center")
  ) %>% 
  select(
    UNIV_ROW_ID, logDate, VQKTERM_CODE
  ) %>% 
  distinct() %>% 
  group_by(UNIV_ROW_ID, VQKTERM_CODE) %>% 
  # ** For each student and each term count number of gym swipes recorded
  tally(name = "logDate") %>% 
  ungroup()
```

## Join Data

With the data cleaned up to be more usable, we now join these tables together into one table that holds all covariates and predicted variables in one place:

```{r}
#| message: false
student_data_tbl <- student_data_tbl %>% 
  
  # * Intramural data
  # ** Using custom function `table_left_join_function`
  table_left_join_function(
    intramural_data_tbl,
    join_by = c("UNIV_ROW_ID", "UWF.ID..", "DEMO_TIME_FRAME", "VQKTERM_CODE"),
    Games.Played
  ) %>% 
  
  # ** Nest all columns from intramural table, we will iterate a sum function
  # ** for each student individually
  nest(INTRAMURAL_GAMES_PLAYED = -names(student_data_tbl)) %>% 
  
  # ** Calculate number of intramural games found, if >= 1: "Yes", if = 0: "No"
  mutate(
    INTRAMURAL_GAMES_PLAYED = map_int(INTRAMURAL_GAMES_PLAYED, ~.x %>% pull(Games.Played) %>% sum),
    across(INTRAMURAL_GAMES_PLAYED, ~replace_na(.x, 0))
  ) %>% 
  mutate(PLAYED_INTRAMURALS_FIRST_FALL = if_else(INTRAMURAL_GAMES_PLAYED > 0, "Yes", "No")
  )

student_data_tbl <- student_data_tbl %>% 
  # * Group Fitness data
  # ** Using custom function `table_left_join_function`
  table_left_join_function(
    group_fitness_data_tbl,
    join_by = c("UNIV_ROW_ID", "Student.ID", "DEMO_TIME_FRAME", "File_Year"),
    Number.of.Formats, Attended
  ) %>% 
 
  # ** Nest all columns from group fitnes table, we will iterate a sum function
  # ** for each student individually
  nest(GROUP_FITNESS = -names(student_data_tbl)) %>% 
  
  # ** Calculate number of group fitness classes attended, if >= 1: "Yes", if = 0: "No"
  mutate(
    GROUP_FITNESS = map(GROUP_FITNESS, function(.y) 
    {
      tibble(
        GROUP_FITNESS_NUM_ATTENDED   = sum(.y$Attended, na.rm = TRUE)
      )
    })
  ) %>% 
  unnest(everything()) %>% 
  mutate(GROUP_FITNESS_FIRST_YEAR = if_else(GROUP_FITNESS_NUM_ATTENDED > 0, "Yes", "No"))

student_data_tbl <- student_data_tbl %>% 
  
  # * Gym Swipe data
  # ** Using custom function `table_left_join_function`
  table_left_join_function(
    user_swipe_tbl,
    join_by = c(rep("UNIV_ROW_ID", 2), "DEMO_TIME_FRAME", "VQKTERM_CODE"),
    logDate
  ) %>% 
  
  # ** Calculate number of gym swipes detected, if >= 1: "Yes", if = 0: "No"
  mutate(
    REC_CENTER_FIRST_FALL = if_else(logDate > 0, "Yes", "No"),
    across(REC_CENTER_FIRST_FALL, ~replace_na(.x, "No"))
  ) %>% 
  select(-logDate)
```

## Data Preparation

With the data available, there were three analyses that were performed for the conference presentation: one measuring APR using the gym swipe variable but only utilizing the 2022 cohort, one measuring APR but not using the gym swipe variable and utilizing the 2018 to 2022 cohorts, and one measuring four year graduation rate while using group fitness data and utilizing the 2018 and 2019 cohorts. This step prepares three data tables to perform separate analyses for each scenario, removing columns that are not used in the analysis (for example, identifier fields, staging fields, fields that were used to feature engineer covariates but are not useful for further analysis, etc.). We use tidyverse syntax to nest these tables, which will prove very much useful as we can write our code more elegantly:

```{r}
#| message: false
student_data_tbl <- tibble(
  names     = c("APR_GYM_SWIPES", "APR_NO_GYM_SWIPES", "FOUR_YEAR_GRAD"),
  data_tbls = list(
    student_data_tbl %>% filter(DEMO_TIME_FRAME == 202208),
    student_data_tbl,
    student_data_tbl %>% filter(between(DEMO_TIME_FRAME, 201808, 201908))
  )
) %>% 
  
  mutate(
    DEP_VAR = map_chr(
      names, ~ifelse(str_detect(.x, "APR"), "APR", "FOURTH_YEAR_DEGS")
    )
  )
  
  # *** Filter data by term/year based on dep var (remove for 4yr grad) ----
# *** Don't use ifelse here as that converts the tibbles to lists which we don't want
# *** Filter data to remove NULL end of term GPAs ----
student_data_tbl <- student_data_tbl %>% 
  
  mutate(
    data_tbls = map2(
      data_tbls, DEP_VAR,
      function(.data_tbls, .DEP_VAR) {
        if (.DEP_VAR == "APR") {
          .data_tbls %>% filter(FIRST_YEAR_EXCLUSIONS == "No")
        } else {
          .data_tbls %>% filter(FOURTH_YEAR_EXCLUSIONS == "No")
        } 
      }
    )
  ) %>% 
  
  mutate(
    data_prep_tbls = map(
      data_tbls, ~.x %>% 
        select(
          -any_of(
            c(
              "UNIV_ROW_ID", "DEMO_TIME_FRAME", "GPA_ENTERING_TERM", "RACE_ETHNICITY"
            )
          ), # Remove these columns if they exist
          -contains("EXCLUSIONS"), # Remove columns that contain "EXCLUSIONS"
          #-ends_with("DEGS"), # Remove columns that end with "DEGS",
          -c(WHITE_FLG : NO_RACE_REPORT_FLG), # Remove individual race flags: we are using `POC_FLG` instead
          -c(BANNER_PROG_DESC) # Remove very specific program information
        )
    ),
    # Remove fields specific to each table 
    data_prep_tbls = map2(
      data_prep_tbls, names, ~
      {
        if (.y == "APR_NO_GYM_SWIPES") {
          .x %>% select(-c(FTPT_IND, FOURTH_YEAR_DEGS, VET_BEN_DESC, contains("GROUP_FITNESS"), REC_CENTER_FIRST_FALL, INTRAMURAL_GAMES_PLAYED))
        } else if (.y == "APR_GYM_SWIPES") {
          .x %>% select(-c(FTPT_IND, FOURTH_YEAR_DEGS, VET_BEN_DESC, contains("GROUP_FITNESS"), INTRAMURAL_GAMES_PLAYED))
        } else {
          .x %>% select(-c(FTPT_IND, APR, VET_BEN_DESC, REC_CENTER_FIRST_FALL, GROUP_FITNESS_NUM_ATTENDED, INTRAMURAL_GAMES_PLAYED))
        } 
      }
    )
  )
```

## Analysis Preparation

Once the data tables were finalized for each analytic scenario, we proceeded to extract relevant features and prepare training and test datasets. For each table, we programmatically identified two sets of covariates: (1) **demographic covariates**, including information such as student background and first-term course load, and (2) **fitness-related covariates**, derived from student engagement with recreational center services, intramural sports, and group fitness classes. To streamline the modeling process, we retained only the dependent variable and the associated covariates in each dataset. We then split each dataset into training and test sets using an 80/20 ratio with a fixed seed for reproducibility. This modular approach allowed us to manage multiple datasets consistently and ensured that each analysis was grounded in well-prepared and comparable subsets of data.

```{r}
#| message: false
student_data_tbl <- student_data_tbl %>% 
  
  mutate(
    
    # *** Get list of covariates (demographics and first fall term load) ----
    DEMOGRAPHIC_COVARIATES = map2(
      data_prep_tbls, DEP_VAR, function(.data_prep_tbls, .DEP_VAR) 
      {
        .data_prep_tbls %>% 
          select(
            -any_of(.DEP_VAR),
            -contains("REC_CENTER"), 
            -contains("INTRAMURAL"), 
            -contains("GROUP_FITNESS")
          ) %>% 
          names()
      }
    ),
    
    # *** Get list of fitness related covariates ----
    FITNESS_COVARIATES_VEC = map(
      data_prep_tbls, ~.x %>% 
        select(
          contains("REC_CENTER"), 
          contains("INTRAMURAL"), 
          contains("GROUP_FITNESS")
        ) %>% 
        select(where(is.character)) %>% 
        names()
    ),
    
    # *** Filter prepped datasets to only include covariates and dependent ----
    data_prep_tbls = pmap(
      list(data_prep_tbls, DEMOGRAPHIC_COVARIATES, FITNESS_COVARIATES_VEC, DEP_VAR),
      function(.x, .covs1, .covs2, .depvar) .x %>% 
        select(all_of(.covs1), all_of(.covs2), all_of(.depvar))
    )
    
  )

student_data_tbl <- student_data_tbl %>% 
  
  mutate(
    
    # *** Set seed ----
    SEED = 123,
    
    # *** Initial splits ----
    data_split = pmap(
      list(data_prep_tbls, SEED),
      function(.data_prep_tbls, .SEED) {
        set.seed(.SEED)
        rsample::initial_split(.data_prep_tbls, prop = 0.8)
      }
    ),
    
    # *** Training set ----
    data_train_tbl = map(data_split, rsample::training),
    
    # *** Test set ----
    data_test_tbl  = map(data_split, rsample::testing)
    
  )
```

## Modeling Workflow

To build consistent and reproducible logistic regression models across all three analyses, we defined a general-purpose modeling pipeline using the `tidymodels` framework. The logistic regression model was specified once using `parsnip`, with a `glm` engine set to classification mode. For each dataset, a custom preprocessing *recipe* was constructed to transform the features appropriately before model training.

The recipe included several key steps:

-   Removal of near-zero variance predictors

-   Mean imputation for missing numeric variables

-   Mode imputation for missing categorical variables

-   Dummy encoding of categorical variables

-   Centering and scaling of predictors, applied conditionally based on the target variable

These recipes were then integrated with the model using `workflows`, creating a tidy and modular pipeline for each dataset. After defining the workflow, we used the `prep()` and `bake()` functions from the `recipes` package to preprocess both training and test data consistently. This approach enabled a clear separation between model specification, preprocessing logic, and dataset handling, ensuring each workflow was tailored to its respective dependent variable while adhering to a shared structure.

```{r}
#| message: false

# * Define model separately (only used for logistic regression) ----
logistic_model <- parsnip::logistic_reg() %>% 
  parsnip::set_engine("glm") %>% 
  parsnip::set_mode("classification")

student_data_tbl <- student_data_tbl %>% 
  
  mutate(
    
    # * Define recipe (done for each data table) ----
    RECIPE  = map2(
      data_train_tbl, DEP_VAR,
      ~ {
        if (.y == "APR") {
          
          recipe(
            reformulate(
              response = .y, 
              termlabels = .x %>% select(-all_of(.y)) %>% names()
            ), 
            data = .x
          ) %>%
            step_nzv(all_predictors(), -all_outcomes()) %>%
            step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%
            step_impute_mode(all_nominal_predictors(), -all_outcomes()) %>%
            step_dummy(all_nominal_predictors(), -all_outcomes()) %>% 
            step_zv(all_predictors(), -all_outcomes()) %>% 
            step_center(all_predictors(), -all_outcomes()) %>% 
            step_scale(all_predictors(), -all_outcomes())
          
        } else {
          
          recipe(
            reformulate(
              response = .y, 
              termlabels = .x %>% select(-all_of(.y)) %>% names()
            ), 
            data = .x
          ) %>%
            step_nzv(all_predictors(), -all_outcomes()) %>%
            step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%
            step_impute_mode(all_nominal_predictors(), -all_outcomes()) %>%
            step_dummy(all_nominal_predictors(), -all_outcomes()) %>%
            step_center(all_numeric_predictors(), -all_outcomes()) %>%
            step_scale(all_numeric_predictors(), -all_outcomes())
          
        }
      }
    ),
    
    # * Set up workflow (done for each data table) ----
    WORKFLOW = map(
      RECIPE, ~workflows::workflow() %>% 
        workflows::add_model(logistic_model) %>% 
        workflows::add_recipe(.x)
    ),
    
    # * Use the prep/bake functions on the training data ----
    BAKED_TRAIN_DATA = map2(
      data_train_tbl, RECIPE, ~ recipes::prep(
        .y, training = .x
      ) %>% 
        recipes::bake(new_data = NULL)
    ),
    
    # * Use the prep/bake functions on the test data ----
    BAKED_TEST_DATA = pmap(
      list(data_train_tbl, RECIPE, data_test_tbl), function(.x, .y, .z) recipes::prep(
        .y, training = .x
      ) %>% 
        recipes::bake(new_data = .z)
    ),
    
  )
```

## Model Fitting

With the workflows prepared and datasets preprocessed, we proceeded to fit the logistic regression models. For each analysis scenario, the model was trained on its respective training set using the previously defined workflow object. To ensure reproducibility, a fixed seed was used for each model fitting process. The dependent variable was explicitly cast to a factor to ensure correct classification behavior during model fitting.

After training, predictions were generated on the corresponding test sets. These predictions were paired with the true class labels to facilitate downstream evaluation. By structuring this step using `pmap()`, we maintained a consistent modeling interface across all analytic scenarios, allowing for scalable and repeatable model execution.

```{r}
#| message: false
student_data_tbl <- student_data_tbl %>% 
  
  mutate(
    
    # * Fit the model based on specifications ----
    FIT_MODEL = pmap(
      list(DEP_VAR, WORKFLOW, data_train_tbl, SEED), 
      function(.DEP_VAR, .WORKFLOW, .data_train_tbl, .seed) {
        set.seed(.seed)
        parsnip::fit(.WORKFLOW, data = .data_train_tbl %>% mutate(across(all_of(.DEP_VAR), as.factor)))
      }
    )
  )

student_data_tbl <- student_data_tbl %>% 
  
  mutate(
    
    # * Get model predictions ----
    PREDICT_MODEL = pmap(
      list(FIT_MODEL, data_test_tbl, DEP_VAR),
      function(.FIT_MODEL, .test_data, .DEP_VAR) {
        .test_data %>% 
          select(all_of(.DEP_VAR)) %>% 
          bind_cols(
            predict(.FIT_MODEL, new_data = .test_data)
          )
      }
    )
    
  )
```

## Model Analysis

After fitting each model and generating predictions, we conducted several diagnostic and interpretive analyses to evaluate model performance and understand feature contributions. For each model, a **confusion matrix** was created using the `uwfir` package, allowing us to assess classification accuracy and identify areas of misclassification. These confusion matrices were converted into interactive summary tables to enhance interpretability and presentation quality.

In addition, we used `broom::tidy()` to extract model coefficients in a tidy format, and a custom function was applied to generate clean summary tables with rounded estimates. These outputs provide transparency into the model's underlying statistical relationships.

To interpret feature importance, we extracted the fitted engine from each workflow and visualized variable influence using the `vip` package. The resulting plots help identify which predictors were most influential in each scenario. Titles and subtitles were dynamically generated to clarify the relationship between feature importance and the respective dependent variable. These plots were styled using `tidyquant::theme_tq()` for visual consistency and clarity.

```{r}
#| message: false
student_data_tbl <- student_data_tbl %>% 
  
  mutate(
    
    CONFUSION_MATRIX       = map(PREDICT_MODEL, uwfir::models_logistic_create_confusion_matrix),
    CONFUSION_MATRIX_TABLE = map(
      CONFUSION_MATRIX, 
      ~.x$metrics %>% 
        models_logistic_create_confusion_matrix_datatable(
          Metric   = ".metric",
          Estimate = ".estimate",
          Percent  = ".pct"
        )
    ),
    
    TIDY_FIT_MODEL     = map(FIT_MODEL, broom::tidy),
    SUMMARY_DATATABLE  = map(
      FIT_MODEL, 
      ~uwfir::models_logistic_create_summary_datatable(
        .x, round = 3
      )
    ),
    EXTRACT_FIT_MODEL  = map(FIT_MODEL, extract_fit_engine),
    
    FEATURE_IMPORTANCE = map(EXTRACT_FIT_MODEL, ~vip::vip(.x, geom = "point")),
    FEATURE_IMPORTANCE = pmap(
      list(FEATURE_IMPORTANCE, DEP_VAR),
      function(.x, .y) .x + 
        labs(
          title    = "Feature Importance",
          subtitle = str_glue(
            "The higher the score, the more important the feature is in predicting {.y}"
          )
        ) +
        tidyquant::theme_tq()
    )
  )
```

## Hyperparameter Tuning

To optimize model performance for the APR prediction scenario, we implemented hyperparameter tuning for the `under_ratio` parameter within the `step_downsample()` function from the `themis` package. This parameter controls the ratio of the minority to majority class after downsampling and plays a critical role in addressing class imbalance in logistic regression.

A regular grid search was performed over 50 values of `under_ratio`, ranging from 0.1 to 5.0. We used 10-fold cross-validation with a fixed seed to ensure consistent and repeatable results across runs. For each fold, a modified recipe was constructed, integrating `step_downsample()` with a tunable parameter, and embedded into a `workflow` object for seamless integration with model training.

The `tune_grid()` function from the `tune` package was used to evaluate model performance across a range of classification metrics including:

-   **Accuracy**

-   **Precision**

-   **Recall**

-   **F1 Score**

-   **ROC AUC**

We collected all tuning results and identified the best-performing `under_ratio` value for each metric using `select_best()`. These optimal values were summarized both in tabular and graphical formats. The resulting plot shows performance across metrics as a function of the under-sampling ratio, with tooltips providing precise values for interactive inspection using `ggplotly()`. This step helped us balance predictive performance and fairness across multiple evaluation dimensions, ensuring the model is not just accurate but also well-calibrated for imbalanced data.

```{r}
#| eval: false
#| message: false
#| warning: false
# * For the step_downsample under_ratio argument ----

# *** Set tuning grid ----
tuning_grid <- dials::grid_regular(under_ratio(range = c(0.1, 5)), levels = 50)

tune_obj <- student_data_tbl %>% 
  filter(DEP_VAR == "APR") %>% 
  
  select(DEP_VAR, data_train_tbl) %>% 
  mutate(
    
    # *** Setting up 10-Fold Cross-Validation ----
    CV_FOLDS = map2(
      data_train_tbl, DEP_VAR,
      function(.data_train_tbl, .DEP_VAR) {
        set.seed(123)
        rsample::vfold_cv(.data_train_tbl)
      }
    ),
    # *** Update recipe to set under_ratio parameter to tune() ----
    TUNE_RECIPE  = map2(
      data_train_tbl, DEP_VAR,
      ~
        recipe(
          reformulate(
            response = .y, 
            termlabels = .x %>% select(-all_of(.y)) %>% names()
          ), 
          data = .x
        ) %>%
        step_nzv(all_predictors(), -all_outcomes()) %>%
        step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%
        step_impute_mode(all_nominal_predictors(), -all_outcomes()) %>%
        step_dummy(all_nominal_predictors(), -all_outcomes()) %>% 
        themis::step_downsample(all_of(.y), under_ratio = tune::tune()) %>% 
        step_zv(all_predictors(), -all_outcomes()) %>%
        step_center(all_predictors(), -all_outcomes()) %>% 
        step_scale(all_predictors(), -all_outcomes())
    ),
    # *** Update workflow with the updated recipe ----
    TUNE_WORKFLOW = map(
      TUNE_RECIPE, ~workflow() %>% 
        add_recipe(.x) %>% 
        add_model(logistic_model)
    ),
    # *** Run tuning grid ----
    TUNE_RESULTS  = map2(
      TUNE_WORKFLOW, CV_FOLDS,
      function(.TUNE_WORKFLOW, .CV_FOLDS) {
        set.seed(123)
        tune::tune_grid(
          .TUNE_WORKFLOW,
          resamples = .CV_FOLDS,
          grid      = tuning_grid,
          metrics   = yardstick::metric_set(
            yardstick::roc_auc,
            yardstick::accuracy,
            yardstick::precision,
            yardstick::recall,
            yardstick::f_meas
          )
        )
      }
    )
    
  )

tuning_eval_obj <- tune_obj %>% 
  
  mutate(
    # *** Collect metrics from tuning ----
    TUNING_METRICS  = map(
      TUNE_RESULTS, workflowsets::collect_metrics
    ),
    # *** Filter to the best value of under-sample ratio for each metric ----
    BEST_OF_METRICS = map2(
      TUNING_METRICS, TUNE_RESULTS, function(.tuning_metrics, .tune_results) .tuning_metrics %>% 
        nest(data = -.metric) %>% 
        mutate(
          test = map(.metric, function(.y) tune::select_best(.tune_results, metric = .y))
        ) %>% 
        select(.metric, test) %>% 
        unnest(everything())
    ),
    BEST_OF_METRICS_TABLE = map(
      BEST_OF_METRICS, 
      ~uwfir::models_general_create_datatable(
        .x, "Best Tuning Parameters", round = 5, `Under Ratio` = "under_ratio", Metric = ".metric", Config = ".config"
      )
    ) ,
    BEST_OF_METRICS_PLOT  = map(
      TUNING_METRICS, ~.x %>%
        mutate(
          .metric = case_when(
            .metric == "accuracy"  ~ "Accuracy",
            .metric == "f_meas"    ~ "F1-Score",
            .metric == "precision" ~ "Precision",
            .metric == "recall"    ~ "Recall",
            .metric == "roc_auc"   ~ "ROC Area Under Curve"
          ),
          across(mean, as.numeric),
          .tooltip = str_glue(
            "Under-sample Ratio: {under_ratio}
            {.metric}: {round(mean, 2)}"
          )
        ) %>%
        ggplot(aes(x = under_ratio, y = mean, group = .metric, color = .metric, text = .tooltip)) +
        geom_point() +
        geom_line() +
        facet_wrap(~ .metric, scales = "free_y") +
        labs(
          x = "Under-sampling Ratio", 
          y = "Metric Value", 
          title = "Performance vs. Under-sampling Ratio for Various Metrics"
        ) +
        tidyquant::theme_tq() +
        tidyquant::scale_fill_tq() +
        tidyquant::scale_color_tq() +
        theme(legend.position = "none")
    ),
    BEST_OF_METRICS_PLOTLY = map(BEST_OF_METRICS_PLOT, ~ggplotly(.x, tooltip = ".tooltip")),
    TUNING_METRICS         = map(
      TUNING_METRICS, ~uwfir::models_general_create_datatable(
        .x, "All Tuning Parameters", round = 5
      )
    )
    
  )
```

## Update Model Recipe

Following hyperparameter tuning, we re-specified and re-fitted the models to incorporate the optimal under-sampling ratios identified for each APR analysis. This step ensures that final model estimates reflect the most effective class balance strategy, rather than the default or untuned approach.

Two of the three models---those predicting **APR**---were refit with distinct `under_ratio` values based on our results (though of course these values will almost certainly differ from what you see with your data):

-   The **APR_GYM_SWIPES** model was assigned an `under_ratio` of **3.9**.

-   The **APR_NO_GYM** model used a more balanced value of **1.4**.

These ratios were incorporated into the preprocessing recipe via the `step_downsample()` function. The third model, which predicts **four-year graduation rate**, was not refit with a downsampling step, as class imbalance was not present and not a priority for that outcome.

To facilitate the refit, we created a new object that preserved only the essential data elements (e.g., training and test sets) and removed outputs from the prior modeling steps. Each model's recipe was updated accordingly, and new workflows were constructed and applied to regenerate baked training and test datasets. This process maintains consistency with earlier modeling logic while explicitly embedding the tuned parameter values for improved performance and fairness.

```{r}
#| message: false
# Recipe is the same as before but we add the step_downsample value for selected modeling scenarios. This is only done for the two APR models, not the four year graduation rate model. And the APR models have separate values selected for under_ratio
student_data_model_refit_tbl <- student_data_tbl %>% 
  
  # As we are refitting models, I will remove objects associated with the previous model
  # and save in another object
  select(names : data_test_tbl) %>% 
  
  mutate(
    
    UNDER_RATIO = if_else(names == "APR_GYP_SWIPES", 3.9, 1.4), # These values selected using F1-scores (hyperparameter tuning)
    
    # * Define recipe (done for each data table) ----
    RECIPE  = pmap(
      list(data_train_tbl, DEP_VAR, UNDER_RATIO),
      function(.x, .y, .under_ratio) {
        if (.y == "APR") {
          
          recipe(
            reformulate(
              response = .y, 
              termlabels = .x %>% select(-all_of(.y)) %>% names()
            ), 
            data = .x
          ) %>%
            step_nzv(all_predictors(), -all_outcomes()) %>%
            step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%
            step_impute_mode(all_nominal_predictors(), -all_outcomes()) %>%
            step_dummy(all_nominal_predictors(), -all_outcomes()) %>% 
            themis::step_downsample(all_of(.y), under_ratio = .under_ratio) %>%
            step_zv(all_predictors(), -all_outcomes()) %>% 
            step_center(all_predictors(), -all_outcomes()) %>% 
            step_scale(all_predictors(), -all_outcomes())
          
        } else {
          
          recipe(
            reformulate(
              response = .y, 
              termlabels = .x %>% select(-all_of(.y)) %>% names()
            ), 
            data = .x
          ) %>%
            step_nzv(all_predictors(), -all_outcomes()) %>%
            step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%
            step_impute_mode(all_nominal_predictors(), -all_outcomes()) %>%
            step_dummy(all_nominal_predictors(), -all_outcomes()) %>%
            step_center(all_numeric_predictors(), -all_outcomes()) %>%
            step_scale(all_numeric_predictors(), -all_outcomes())
          
        }
      }
    ),
    
    # * Set up workflow (done for each data table) ----
    WORKFLOW = map(
      RECIPE, ~workflows::workflow() %>% 
        workflows::add_model(logistic_model) %>% 
        workflows::add_recipe(.x)
    ),
    
    # * Use the prep/bake functions on the training data ----
    BAKED_TRAIN_DATA = map2(
      data_train_tbl, RECIPE, ~ recipes::prep(
        .y, training = .x
      ) %>% 
        recipes::bake(new_data = NULL)
    ),
    
    # * Use the prep/bake functions on the test data ----
    BAKED_TEST_DATA = pmap(
      list(data_train_tbl, RECIPE, data_test_tbl), function(.x, .y, .z) recipes::prep(
        .y, training = .x
      ) %>% 
        recipes::bake(new_data = .z)
    ),
    
  )
```

## Model Refitting

With the optimized preprocessing workflows in place---including tuned downsampling ratios for the APR models---we proceeded to refit each model on the full training data. Using the same logistic regression specification as before, we applied the updated workflows to generate new final model fits. A fixed seed was maintained for reproducibility.

Each dependent variable was cast as a factor to ensure proper handling by the classification model, and model training was conducted independently for each scenario. After fitting, we generated predictions on the respective test datasets. As before, each prediction output was paired with the true class label, forming the basis for final performance evaluation.

This step finalized the modeling pipeline with refined inputs and optimal preprocessing, ensuring that the models reflect the best-performing configurations identified during tuning while maintaining consistency in workflow structure and reproducibility.

```{r}
#| message: false
student_data_model_refit_tbl <- student_data_model_refit_tbl %>% 
  
  mutate(
    
    # * Fit the model based on specifications ----
    FIT_MODEL = pmap(
      list(DEP_VAR, WORKFLOW, data_train_tbl, SEED), 
      function(.DEP_VAR, .WORKFLOW, .data_train_tbl, .seed) {
        set.seed(.seed)
        parsnip::fit(.WORKFLOW, data = .data_train_tbl %>% mutate(across(all_of(.DEP_VAR), as.factor)))
      }
    )
  )

student_data_model_refit_tbl <- student_data_model_refit_tbl %>% 
  
  mutate(
    
    # * Get model predictions ----
    PREDICT_MODEL = pmap(
      list(FIT_MODEL, data_test_tbl, DEP_VAR),
      function(.FIT_MODEL, .test_data, .DEP_VAR) {
        .test_data %>% 
          select(all_of(.DEP_VAR)) %>% 
          bind_cols(
            predict(.FIT_MODEL, new_data = .test_data)
          )
      }
    )
    
  )
```
