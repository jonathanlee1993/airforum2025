[
  {
    "objectID": "air2025.html",
    "href": "air2025.html",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "",
    "text": "This project investigates the relationships between student usage of the University of West Florida (UWF) Health Leisure and Sports (HLS) Facility recreation center, group fitness activities, and intramural sports participation with student performance indicators such as full-time first time in college (FTIC) four year graduation rate and full-time FTIC academic progress rate (defined as a student being enrolled in the second fall with a cumulative GPA of 2.0 or higher) using logistic regression. The goal is to identify the strength of any existing relationships between those students performance indicators with any HLS usage data, after taking a variety of student demographic and performance factors into account.\nTo improve model performance and handle potential class imbalance, we apply hyperparameter tuning using the `tidymodels` framework and evaluate model performance with appropriate metrics.\nThis guide is written as a reproducible Quarto document for sharing, documentation, and educational purposes."
  },
  {
    "objectID": "air2025.html#setup",
    "href": "air2025.html#setup",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "Setup",
    "text": "Setup\nWe’ll use `tidymodels` as the modeling framework and include additional packages for cleaning, preprocessing, and visualization. The following code block gives a comprehensive list of packages that you will need to load (or install if you have not installed them):\n\n\n\n\n\nShow Code\n# Optional: install these if not already installed\n# install.packages(\n#   c(\n#     \"tidymodels\", \"themis\", \"janitor\", \"skimr\", \"vip\", \"correlationfunnel\",\n#     \"tidyverse\", \"ggplot\", \"plotly\", \"DT\", \"shiny\", \"shinyWidgets\", \"shinyjs\"\n#   )\n# )\n# \n# Install my custom package from GitHub\n# This will give you access to a few custom functions I wrote for this analysis\n# devtools::install_github(\"\")\n\n# LOAD LIBRARIES\n\n# Custom package\n# library()\n\n# Tidyverse\nlibrary(tidyverse)         # Core data science packages for data manipulation, visualization, and piping\nlibrary(tidymodels)        # Framework for modeling and machine learning using the tidyverse philosophy\n\n# Modeling (most loaded via tidyverse or tidymodels, but explicitly listed here for clarity)\nlibrary(broom)             # Converts model objects into tidy tibbles\nlibrary(parsnip)           # Unified interface to create and fit models\nlibrary(themis)            # Tools for dealing with class imbalance (e.g., SMOTE, downsampling)\nlibrary(rsample)           # Functions for resampling (e.g., train/test splits, cross-validation)    \nlibrary(recipes)           # Preprocessing steps like normalization, encoding, feature engineering    \nlibrary(workflows)         # Combines preprocessing and modeling into a single workflow\nlibrary(workflowsets)      # Enables tuning across multiple workflows simultaneously\nlibrary(dials)             # Defines hyperparameters for tuning\nlibrary(tune)              # Tools for hyperparameter tuning and grid search\nlibrary(yardstick)         # Metrics for evaluating model performance (accuracy, ROC AUC, etc.)\n\n# Plots\nlibrary(vip)               # For generating variable importance plots for ML models\nlibrary(ggplot2)           # Grammar of graphics system used to create static plots\nlibrary(plotly)            # Makes ggplot2 graphs interactive and supports dynamic web visualizations\nlibrary(tidyquant)         # Combines tidyverse with financial and time series analysis tools, but used here for plotting purposes\n\n# Shiny\nlibrary(shiny)             # Web application framework for R\nlibrary(shinyWidgets)      # Custom UI widgets to enhance Shiny app interactivity and design\nlibrary(shinyjs)           # Enables use of JavaScript functions in Shiny apps (e.g., show/hide elements)\n\n# Plotting Elements\nlibrary(DT)                # Creates interactive data tables in Shiny or R Markdown\nlibrary(correlationfunnel) # Helps visualize and explore correlations between features and binary outcomes"
  },
  {
    "objectID": "air2025.html#load-data",
    "href": "air2025.html#load-data",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "Load Data",
    "text": "Load Data\nWe begin by loading the required datasets. For portability, we assume the working directory is the root of the project folder, and all data files are stored in the `./data/` directory.\n\n\nShow Code\n# * Student data: FT FTIC Cohort students with outcome and covariate data included\nstudent_data_tbl <- readr::read_rds(\"./data/student_data_tbl.rds\") %>% \n  filter(between(DEMO_TIME_FRAME, 201808, 202208))\n\n# * Rec Center Swipe data\nuser_swipe_tbl <- readr::read_rds(\"./data/user_swipe_tbl.rds\")\n\n# * Intramurals data\nintramural_data_tbl <- readr::read_rds(\"./data/intramural_data_tbl.rds\")\n\n# * Group Fitness data\ngroup_fitness_data_tbl <- readr::read_rds(\"./data/group_fitness_data_tbl.rds\")"
  },
  {
    "objectID": "air2025.html#clean-data",
    "href": "air2025.html#clean-data",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "Clean Data",
    "text": "Clean Data\nWe perform a few steps to clean the gym swipe, intramural, and group fitness data so that they are easier to use and easier to join to the student data, and to select the relevant gym swipe data/filter irrelevant data (such as blocked entries):\n\n\nShow Code\n# * Intramural: Winter Year should have vqkterm_code manually assigned to (File_Year+1)01\n# * vqkterm_code is a field associated with an academic term: for example, 202401 = spring 2024\nintramural_data_tbl <- intramural_data_tbl %>% \n  mutate(\n    VQKTERM_CODE = if_else(is.na(VQKTERM_CODE), as.character(as.integer(str_glue(\"{File_Year}08\"))+93), VQKTERM_CODE)\n  ) %>% \n  # ** Change Student ID and Term Code types to Integer\n  mutate(\n    across(c(UWF.ID.., VQKTERM_CODE), as.integer)\n  )\n\n# Group Fitness: Change Student ID and Year types to Integer, and convert Year to Fall Term [File_Year]08\ngroup_fitness_data_tbl <- group_fitness_data_tbl %>% \n  mutate(\n    across(c(File_Year, Student.ID), as.integer),\n    across(File_Year, ~100*.x + 8)\n  )\n\n# * Rec Center Swipes: Remove blocked entries, get entry/exit/center data ----\nuser_swipe_tbl <- user_swipe_tbl %>% \n  filter(\n    blocked == 0,\n    str_detect(source, \"Entr|Exit|-center\")\n  ) %>% \n  select(\n    UNIV_ROW_ID, logDate, VQKTERM_CODE\n  ) %>% \n  distinct() %>% \n  group_by(UNIV_ROW_ID, VQKTERM_CODE) %>% \n  # ** For each student and each term count number of gym swipes recorded\n  tally(name = \"logDate\") %>% \n  ungroup()"
  },
  {
    "objectID": "air2025.html#join-data",
    "href": "air2025.html#join-data",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "Join Data",
    "text": "Join Data\nWith the data cleaned up to be more usable, we now join these tables together into one table that holds all covariates and predicted variables in one place:\n\n\nShow Code\nstudent_data_tbl <- student_data_tbl %>% \n  \n  # * Intramural data\n  # ** Using custom function `table_left_join_function`\n  table_left_join_function(\n    intramural_data_tbl,\n    join_by = c(\"UNIV_ROW_ID\", \"UWF.ID..\", \"DEMO_TIME_FRAME\", \"VQKTERM_CODE\"),\n    Games.Played\n  ) %>% \n  \n  # ** Nest all columns from intramural table, we will iterate a sum function\n  # ** for each student individually\n  nest(INTRAMURAL_GAMES_PLAYED = -names(student_data_tbl)) %>% \n  \n  # ** Calculate number of intramural games found, if >= 1: \"Yes\", if = 0: \"No\"\n  mutate(\n    INTRAMURAL_GAMES_PLAYED = map_int(INTRAMURAL_GAMES_PLAYED, ~.x %>% pull(Games.Played) %>% sum),\n    across(INTRAMURAL_GAMES_PLAYED, ~replace_na(.x, 0))\n  ) %>% \n  mutate(PLAYED_INTRAMURALS_FIRST_FALL = if_else(INTRAMURAL_GAMES_PLAYED > 0, \"Yes\", \"No\")\n  )\n\nstudent_data_tbl <- student_data_tbl %>% \n  # * Group Fitness data\n  # ** Using custom function `table_left_join_function`\n  table_left_join_function(\n    group_fitness_data_tbl,\n    join_by = c(\"UNIV_ROW_ID\", \"Student.ID\", \"DEMO_TIME_FRAME\", \"File_Year\"),\n    Number.of.Formats, Attended\n  ) %>% \n \n  # ** Nest all columns from group fitnes table, we will iterate a sum function\n  # ** for each student individually\n  nest(GROUP_FITNESS = -names(student_data_tbl)) %>% \n  \n  # ** Calculate number of group fitness classes attended, if >= 1: \"Yes\", if = 0: \"No\"\n  mutate(\n    GROUP_FITNESS = map(GROUP_FITNESS, function(.y) \n    {\n      tibble(\n        GROUP_FITNESS_NUM_ATTENDED   = sum(.y$Attended, na.rm = TRUE)\n      )\n    })\n  ) %>% \n  unnest(everything()) %>% \n  mutate(GROUP_FITNESS_FIRST_YEAR = if_else(GROUP_FITNESS_NUM_ATTENDED > 0, \"Yes\", \"No\"))\n\nstudent_data_tbl <- student_data_tbl %>% \n  \n  # * Gym Swipe data\n  # ** Using custom function `table_left_join_function`\n  table_left_join_function(\n    user_swipe_tbl,\n    join_by = c(rep(\"UNIV_ROW_ID\", 2), \"DEMO_TIME_FRAME\", \"VQKTERM_CODE\"),\n    logDate\n  ) %>% \n  \n  # ** Calculate number of gym swipes detected, if >= 1: \"Yes\", if = 0: \"No\"\n  mutate(\n    REC_CENTER_FIRST_FALL = if_else(logDate > 0, \"Yes\", \"No\"),\n    across(REC_CENTER_FIRST_FALL, ~replace_na(.x, \"No\"))\n  ) %>% \n  select(-logDate)"
  },
  {
    "objectID": "air2025.html#data-preparation",
    "href": "air2025.html#data-preparation",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "Data Preparation",
    "text": "Data Preparation\nWith the data available, there were three analyses that were performed for the conference presentation: one measuring APR using the gym swipe variable but only utilizing the 2022 cohort, one measuring APR but not using the gym swipe variable and utilizing the 2018 to 2022 cohorts, and one measuring four year graduation rate while using group fitness data and utilizing the 2018 and 2019 cohorts. This step prepares three data tables to perform separate analyses for each scenario, removing columns that are not used in the analysis (for example, identifier fields, staging fields, fields that were used to feature engineer covariates but are not useful for further analysis, etc.). We use tidyverse syntax to nest these tables, which will prove very much useful as we can write our code more elegantly:\n\n\nShow Code\nstudent_data_tbl <- tibble(\n  names     = c(\"APR_GYM_SWIPES\", \"APR_NO_GYM_SWIPES\", \"FOUR_YEAR_GRAD\"),\n  data_tbls = list(\n    student_data_tbl %>% filter(DEMO_TIME_FRAME == 202208),\n    student_data_tbl,\n    student_data_tbl %>% filter(between(DEMO_TIME_FRAME, 201808, 201908))\n  )\n) %>% \n  \n  mutate(\n    DEP_VAR = map_chr(\n      names, ~ifelse(str_detect(.x, \"APR\"), \"APR\", \"FOURTH_YEAR_DEGS\")\n    )\n  )\n  \n  # *** Filter data by term/year based on dep var (remove for 4yr grad) ----\n# *** Don't use ifelse here as that converts the tibbles to lists which we don't want\n# *** Filter data to remove NULL end of term GPAs ----\nstudent_data_tbl <- student_data_tbl %>% \n  \n  mutate(\n    data_tbls = map2(\n      data_tbls, DEP_VAR,\n      function(.data_tbls, .DEP_VAR) {\n        if (.DEP_VAR == \"APR\") {\n          .data_tbls %>% filter(FIRST_YEAR_EXCLUSIONS == \"No\")\n        } else {\n          .data_tbls %>% filter(FOURTH_YEAR_EXCLUSIONS == \"No\")\n        } \n      }\n    )\n  ) %>% \n  \n  mutate(\n    data_prep_tbls = map(\n      data_tbls, ~.x %>% \n        select(\n          -any_of(\n            c(\n              \"UNIV_ROW_ID\", \"DEMO_TIME_FRAME\", \"GPA_ENTERING_TERM\", \"RACE_ETHNICITY\"\n            )\n          ), # Remove these columns if they exist\n          -contains(\"EXCLUSIONS\"), # Remove columns that contain \"EXCLUSIONS\"\n          #-ends_with(\"DEGS\"), # Remove columns that end with \"DEGS\",\n          -c(WHITE_FLG : NO_RACE_REPORT_FLG), # Remove individual race flags: we are using `POC_FLG` instead\n          -c(BANNER_PROG_DESC) # Remove very specific program information\n        )\n    ),\n    # Remove fields specific to each table \n    data_prep_tbls = map2(\n      data_prep_tbls, names, ~\n      {\n        if (.y == \"APR_NO_GYM_SWIPES\") {\n          .x %>% select(-c(FTPT_IND, FOURTH_YEAR_DEGS, VET_BEN_DESC, contains(\"GROUP_FITNESS\"), REC_CENTER_FIRST_FALL, INTRAMURAL_GAMES_PLAYED))\n        } else if (.y == \"APR_GYM_SWIPES\") {\n          .x %>% select(-c(FTPT_IND, FOURTH_YEAR_DEGS, VET_BEN_DESC, contains(\"GROUP_FITNESS\"), INTRAMURAL_GAMES_PLAYED))\n        } else {\n          .x %>% select(-c(FTPT_IND, APR, VET_BEN_DESC, REC_CENTER_FIRST_FALL, GROUP_FITNESS_NUM_ATTENDED, INTRAMURAL_GAMES_PLAYED))\n        } \n      }\n    )\n  )"
  },
  {
    "objectID": "air2025.html#analysis-preparation",
    "href": "air2025.html#analysis-preparation",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "Analysis Preparation",
    "text": "Analysis Preparation\nOnce the data tables were finalized for each analytic scenario, we proceeded to extract relevant features and prepare training and test datasets. For each table, we programmatically identified two sets of covariates: (1) demographic covariates, including information such as student background and first-term course load, and (2) fitness-related covariates, derived from student engagement with recreational center services, intramural sports, and group fitness classes. To streamline the modeling process, we retained only the dependent variable and the associated covariates in each dataset. We then split each dataset into training and test sets using an 80/20 ratio with a fixed seed for reproducibility. This modular approach allowed us to manage multiple datasets consistently and ensured that each analysis was grounded in well-prepared and comparable subsets of data.\n\n\nShow Code\nstudent_data_tbl <- student_data_tbl %>% \n  \n  mutate(\n    \n    # *** Get list of covariates (demographics and first fall term load) ----\n    DEMOGRAPHIC_COVARIATES = map2(\n      data_prep_tbls, DEP_VAR, function(.data_prep_tbls, .DEP_VAR) \n      {\n        .data_prep_tbls %>% \n          select(\n            -any_of(.DEP_VAR),\n            -contains(\"REC_CENTER\"), \n            -contains(\"INTRAMURAL\"), \n            -contains(\"GROUP_FITNESS\")\n          ) %>% \n          names()\n      }\n    ),\n    \n    # *** Get list of fitness related covariates ----\n    FITNESS_COVARIATES_VEC = map(\n      data_prep_tbls, ~.x %>% \n        select(\n          contains(\"REC_CENTER\"), \n          contains(\"INTRAMURAL\"), \n          contains(\"GROUP_FITNESS\")\n        ) %>% \n        select(where(is.character)) %>% \n        names()\n    ),\n    \n    # *** Filter prepped datasets to only include covariates and dependent ----\n    data_prep_tbls = pmap(\n      list(data_prep_tbls, DEMOGRAPHIC_COVARIATES, FITNESS_COVARIATES_VEC, DEP_VAR),\n      function(.x, .covs1, .covs2, .depvar) .x %>% \n        select(all_of(.covs1), all_of(.covs2), all_of(.depvar))\n    )\n    \n  )\n\nstudent_data_tbl <- student_data_tbl %>% \n  \n  mutate(\n    \n    # *** Set seed ----\n    SEED = 123,\n    \n    # *** Initial splits ----\n    data_split = pmap(\n      list(data_prep_tbls, SEED),\n      function(.data_prep_tbls, .SEED) {\n        set.seed(.SEED)\n        rsample::initial_split(.data_prep_tbls, prop = 0.8)\n      }\n    ),\n    \n    # *** Training set ----\n    data_train_tbl = map(data_split, rsample::training),\n    \n    # *** Test set ----\n    data_test_tbl  = map(data_split, rsample::testing)\n    \n  )"
  },
  {
    "objectID": "air2025.html#modeling-workflow",
    "href": "air2025.html#modeling-workflow",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "Modeling Workflow",
    "text": "Modeling Workflow\nTo build consistent and reproducible logistic regression models across all three analyses, we defined a general-purpose modeling pipeline using the tidymodels framework. The logistic regression model was specified once using parsnip, with a glm engine set to classification mode. For each dataset, a custom preprocessing recipe was constructed to transform the features appropriately before model training.\nThe recipe included several key steps:\n\nRemoval of near-zero variance predictors\nMean imputation for missing numeric variables\nMode imputation for missing categorical variables\nDummy encoding of categorical variables\nCentering and scaling of predictors, applied conditionally based on the target variable\n\nThese recipes were then integrated with the model using workflows, creating a tidy and modular pipeline for each dataset. After defining the workflow, we used the prep() and bake() functions from the recipes package to preprocess both training and test data consistently. This approach enabled a clear separation between model specification, preprocessing logic, and dataset handling, ensuring each workflow was tailored to its respective dependent variable while adhering to a shared structure.\n\n\nShow Code\n# * Define model separately (only used for logistic regression) ----\nlogistic_model <- parsnip::logistic_reg() %>% \n  parsnip::set_engine(\"glm\") %>% \n  parsnip::set_mode(\"classification\")\n\nstudent_data_tbl <- student_data_tbl %>% \n  \n  mutate(\n    \n    # * Define recipe (done for each data table) ----\n    RECIPE  = map2(\n      data_train_tbl, DEP_VAR,\n      ~ {\n        if (.y == \"APR\") {\n          \n          recipe(\n            reformulate(\n              response = .y, \n              termlabels = .x %>% select(-all_of(.y)) %>% names()\n            ), \n            data = .x\n          ) %>%\n            step_nzv(all_predictors(), -all_outcomes()) %>%\n            step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%\n            step_impute_mode(all_nominal_predictors(), -all_outcomes()) %>%\n            step_dummy(all_nominal_predictors(), -all_outcomes()) %>% \n            step_zv(all_predictors(), -all_outcomes()) %>% \n            step_center(all_predictors(), -all_outcomes()) %>% \n            step_scale(all_predictors(), -all_outcomes())\n          \n        } else {\n          \n          recipe(\n            reformulate(\n              response = .y, \n              termlabels = .x %>% select(-all_of(.y)) %>% names()\n            ), \n            data = .x\n          ) %>%\n            step_nzv(all_predictors(), -all_outcomes()) %>%\n            step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%\n            step_impute_mode(all_nominal_predictors(), -all_outcomes()) %>%\n            step_dummy(all_nominal_predictors(), -all_outcomes()) %>%\n            step_center(all_numeric_predictors(), -all_outcomes()) %>%\n            step_scale(all_numeric_predictors(), -all_outcomes())\n          \n        }\n      }\n    ),\n    \n    # * Set up workflow (done for each data table) ----\n    WORKFLOW = map(\n      RECIPE, ~workflows::workflow() %>% \n        workflows::add_model(logistic_model) %>% \n        workflows::add_recipe(.x)\n    ),\n    \n    # * Use the prep/bake functions on the training data ----\n    BAKED_TRAIN_DATA = map2(\n      data_train_tbl, RECIPE, ~ recipes::prep(\n        .y, training = .x\n      ) %>% \n        recipes::bake(new_data = NULL)\n    ),\n    \n    # * Use the prep/bake functions on the test data ----\n    BAKED_TEST_DATA = pmap(\n      list(data_train_tbl, RECIPE, data_test_tbl), function(.x, .y, .z) recipes::prep(\n        .y, training = .x\n      ) %>% \n        recipes::bake(new_data = .z)\n    ),\n    \n  )"
  },
  {
    "objectID": "air2025.html#model-fitting",
    "href": "air2025.html#model-fitting",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "Model Fitting",
    "text": "Model Fitting\nWith the workflows prepared and datasets preprocessed, we proceeded to fit the logistic regression models. For each analysis scenario, the model was trained on its respective training set using the previously defined workflow object. To ensure reproducibility, a fixed seed was used for each model fitting process. The dependent variable was explicitly cast to a factor to ensure correct classification behavior during model fitting.\nAfter training, predictions were generated on the corresponding test sets. These predictions were paired with the true class labels to facilitate downstream evaluation. By structuring this step using pmap(), we maintained a consistent modeling interface across all analytic scenarios, allowing for scalable and repeatable model execution.\n\n\nShow Code\nstudent_data_tbl <- student_data_tbl %>% \n  \n  mutate(\n    \n    # * Fit the model based on specifications ----\n    FIT_MODEL = pmap(\n      list(DEP_VAR, WORKFLOW, data_train_tbl, SEED), \n      function(.DEP_VAR, .WORKFLOW, .data_train_tbl, .seed) {\n        set.seed(.seed)\n        parsnip::fit(.WORKFLOW, data = .data_train_tbl %>% mutate(across(all_of(.DEP_VAR), as.factor)))\n      }\n    )\n  )\n\nstudent_data_tbl <- student_data_tbl %>% \n  \n  mutate(\n    \n    # * Get model predictions ----\n    PREDICT_MODEL = pmap(\n      list(FIT_MODEL, data_test_tbl, DEP_VAR),\n      function(.FIT_MODEL, .test_data, .DEP_VAR) {\n        .test_data %>% \n          select(all_of(.DEP_VAR)) %>% \n          bind_cols(\n            predict(.FIT_MODEL, new_data = .test_data)\n          )\n      }\n    )\n    \n  )"
  },
  {
    "objectID": "air2025.html#model-analysis",
    "href": "air2025.html#model-analysis",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "Model Analysis",
    "text": "Model Analysis\nAfter fitting each model and generating predictions, we conducted several diagnostic and interpretive analyses to evaluate model performance and understand feature contributions. For each model, a confusion matrix was created using the uwfir package, allowing us to assess classification accuracy and identify areas of misclassification. These confusion matrices were converted into interactive summary tables to enhance interpretability and presentation quality.\nIn addition, we used broom::tidy() to extract model coefficients in a tidy format, and a custom function was applied to generate clean summary tables with rounded estimates. These outputs provide transparency into the model’s underlying statistical relationships.\nTo interpret feature importance, we extracted the fitted engine from each workflow and visualized variable influence using the vip package. The resulting plots help identify which predictors were most influential in each scenario. Titles and subtitles were dynamically generated to clarify the relationship between feature importance and the respective dependent variable. These plots were styled using tidyquant::theme_tq() for visual consistency and clarity.\n\n\nShow Code\nstudent_data_tbl <- student_data_tbl %>% \n  \n  mutate(\n    \n    CONFUSION_MATRIX       = map(PREDICT_MODEL, uwfir::models_logistic_create_confusion_matrix),\n    CONFUSION_MATRIX_TABLE = map(\n      CONFUSION_MATRIX, \n      ~.x$metrics %>% \n        models_logistic_create_confusion_matrix_datatable(\n          Metric   = \".metric\",\n          Estimate = \".estimate\",\n          Percent  = \".pct\"\n        )\n    ),\n    \n    TIDY_FIT_MODEL     = map(FIT_MODEL, broom::tidy),\n    SUMMARY_DATATABLE  = map(\n      FIT_MODEL, \n      ~uwfir::models_logistic_create_summary_datatable(\n        .x, round = 3\n      )\n    ),\n    EXTRACT_FIT_MODEL  = map(FIT_MODEL, extract_fit_engine),\n    \n    FEATURE_IMPORTANCE = map(EXTRACT_FIT_MODEL, ~vip::vip(.x, geom = \"point\")),\n    FEATURE_IMPORTANCE = pmap(\n      list(FEATURE_IMPORTANCE, DEP_VAR),\n      function(.x, .y) .x + \n        labs(\n          title    = \"Feature Importance\",\n          subtitle = str_glue(\n            \"The higher the score, the more important the feature is in predicting {.y}\"\n          )\n        ) +\n        tidyquant::theme_tq()\n    )\n  )"
  },
  {
    "objectID": "air2025.html#hyperparameter-tuning",
    "href": "air2025.html#hyperparameter-tuning",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\nTo optimize model performance for the APR prediction scenario, we implemented hyperparameter tuning for the under_ratio parameter within the step_downsample() function from the themis package. This parameter controls the ratio of the minority to majority class after downsampling and plays a critical role in addressing class imbalance in logistic regression.\nA regular grid search was performed over 50 values of under_ratio, ranging from 0.1 to 5.0. We used 10-fold cross-validation with a fixed seed to ensure consistent and repeatable results across runs. For each fold, a modified recipe was constructed, integrating step_downsample() with a tunable parameter, and embedded into a workflow object for seamless integration with model training.\nThe tune_grid() function from the tune package was used to evaluate model performance across a range of classification metrics including:\n\nAccuracy\nPrecision\nRecall\nF1 Score\nROC AUC\n\nWe collected all tuning results and identified the best-performing under_ratio value for each metric using select_best(). These optimal values were summarized both in tabular and graphical formats. The resulting plot shows performance across metrics as a function of the under-sampling ratio, with tooltips providing precise values for interactive inspection using ggplotly(). This step helped us balance predictive performance and fairness across multiple evaluation dimensions, ensuring the model is not just accurate but also well-calibrated for imbalanced data.\n\n\nShow Code\n# * For the step_downsample under_ratio argument ----\n\n# *** Set tuning grid ----\ntuning_grid <- dials::grid_regular(under_ratio(range = c(0.1, 5)), levels = 50)\n\ntune_obj <- student_data_tbl %>% \n  filter(DEP_VAR == \"APR\") %>% \n  \n  select(DEP_VAR, data_train_tbl) %>% \n  mutate(\n    \n    # *** Setting up 10-Fold Cross-Validation ----\n    CV_FOLDS = map2(\n      data_train_tbl, DEP_VAR,\n      function(.data_train_tbl, .DEP_VAR) {\n        set.seed(123)\n        rsample::vfold_cv(.data_train_tbl)\n      }\n    ),\n    # *** Update recipe to set under_ratio parameter to tune() ----\n    TUNE_RECIPE  = map2(\n      data_train_tbl, DEP_VAR,\n      ~\n        recipe(\n          reformulate(\n            response = .y, \n            termlabels = .x %>% select(-all_of(.y)) %>% names()\n          ), \n          data = .x\n        ) %>%\n        step_nzv(all_predictors(), -all_outcomes()) %>%\n        step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%\n        step_impute_mode(all_nominal_predictors(), -all_outcomes()) %>%\n        step_dummy(all_nominal_predictors(), -all_outcomes()) %>% \n        themis::step_downsample(all_of(.y), under_ratio = tune::tune()) %>% \n        step_zv(all_predictors(), -all_outcomes()) %>%\n        step_center(all_predictors(), -all_outcomes()) %>% \n        step_scale(all_predictors(), -all_outcomes())\n    ),\n    # *** Update workflow with the updated recipe ----\n    TUNE_WORKFLOW = map(\n      TUNE_RECIPE, ~workflow() %>% \n        add_recipe(.x) %>% \n        add_model(logistic_model)\n    ),\n    # *** Run tuning grid ----\n    TUNE_RESULTS  = map2(\n      TUNE_WORKFLOW, CV_FOLDS,\n      function(.TUNE_WORKFLOW, .CV_FOLDS) {\n        set.seed(123)\n        tune::tune_grid(\n          .TUNE_WORKFLOW,\n          resamples = .CV_FOLDS,\n          grid      = tuning_grid,\n          metrics   = yardstick::metric_set(\n            yardstick::roc_auc,\n            yardstick::accuracy,\n            yardstick::precision,\n            yardstick::recall,\n            yardstick::f_meas\n          )\n        )\n      }\n    )\n    \n  )\n\ntuning_eval_obj <- tune_obj %>% \n  \n  mutate(\n    # *** Collect metrics from tuning ----\n    TUNING_METRICS  = map(\n      TUNE_RESULTS, workflowsets::collect_metrics\n    ),\n    # *** Filter to the best value of under-sample ratio for each metric ----\n    BEST_OF_METRICS = map2(\n      TUNING_METRICS, TUNE_RESULTS, function(.tuning_metrics, .tune_results) .tuning_metrics %>% \n        nest(data = -.metric) %>% \n        mutate(\n          test = map(.metric, function(.y) tune::select_best(.tune_results, metric = .y))\n        ) %>% \n        select(.metric, test) %>% \n        unnest(everything())\n    ),\n    BEST_OF_METRICS_TABLE = map(\n      BEST_OF_METRICS, \n      ~uwfir::models_general_create_datatable(\n        .x, \"Best Tuning Parameters\", round = 5, `Under Ratio` = \"under_ratio\", Metric = \".metric\", Config = \".config\"\n      )\n    ) ,\n    BEST_OF_METRICS_PLOT  = map(\n      TUNING_METRICS, ~.x %>%\n        mutate(\n          .metric = case_when(\n            .metric == \"accuracy\"  ~ \"Accuracy\",\n            .metric == \"f_meas\"    ~ \"F1-Score\",\n            .metric == \"precision\" ~ \"Precision\",\n            .metric == \"recall\"    ~ \"Recall\",\n            .metric == \"roc_auc\"   ~ \"ROC Area Under Curve\"\n          ),\n          across(mean, as.numeric),\n          .tooltip = str_glue(\n            \"Under-sample Ratio: {under_ratio}\n            {.metric}: {round(mean, 2)}\"\n          )\n        ) %>%\n        ggplot(aes(x = under_ratio, y = mean, group = .metric, color = .metric, text = .tooltip)) +\n        geom_point() +\n        geom_line() +\n        facet_wrap(~ .metric, scales = \"free_y\") +\n        labs(\n          x = \"Under-sampling Ratio\", \n          y = \"Metric Value\", \n          title = \"Performance vs. Under-sampling Ratio for Various Metrics\"\n        ) +\n        tidyquant::theme_tq() +\n        tidyquant::scale_fill_tq() +\n        tidyquant::scale_color_tq() +\n        theme(legend.position = \"none\")\n    ),\n    BEST_OF_METRICS_PLOTLY = map(BEST_OF_METRICS_PLOT, ~ggplotly(.x, tooltip = \".tooltip\")),\n    TUNING_METRICS         = map(\n      TUNING_METRICS, ~uwfir::models_general_create_datatable(\n        .x, \"All Tuning Parameters\", round = 5\n      )\n    )\n    \n  )"
  },
  {
    "objectID": "air2025.html#update-model-recipe",
    "href": "air2025.html#update-model-recipe",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "Update Model Recipe",
    "text": "Update Model Recipe\nFollowing hyperparameter tuning, we re-specified and re-fitted the models to incorporate the optimal under-sampling ratios identified for each APR analysis. This step ensures that final model estimates reflect the most effective class balance strategy, rather than the default or untuned approach.\nTwo of the three models—those predicting APR—were refit with distinct under_ratio values based on our results (though of course these values will almost certainly differ from what you see with your data):\n\nThe APR_GYM_SWIPES model was assigned an under_ratio of 3.9.\nThe APR_NO_GYM model used a more balanced value of 1.4.\n\nThese ratios were incorporated into the preprocessing recipe via the step_downsample() function. The third model, which predicts four-year graduation rate, was not refit with a downsampling step, as class imbalance was not present and not a priority for that outcome.\nTo facilitate the refit, we created a new object that preserved only the essential data elements (e.g., training and test sets) and removed outputs from the prior modeling steps. Each model’s recipe was updated accordingly, and new workflows were constructed and applied to regenerate baked training and test datasets. This process maintains consistency with earlier modeling logic while explicitly embedding the tuned parameter values for improved performance and fairness.\n\n\nShow Code\n# Recipe is the same as before but we add the step_downsample value for selected modeling scenarios. This is only done for the two APR models, not the four year graduation rate model. And the APR models have separate values selected for under_ratio\nstudent_data_model_refit_tbl <- student_data_tbl %>% \n  \n  # As we are refitting models, I will remove objects associated with the previous model\n  # and save in another object\n  select(names : data_test_tbl) %>% \n  \n  mutate(\n    \n    UNDER_RATIO = if_else(names == \"APR_GYP_SWIPES\", 3.9, 1.4), # These values selected using F1-scores (hyperparameter tuning)\n    \n    # * Define recipe (done for each data table) ----\n    RECIPE  = pmap(\n      list(data_train_tbl, DEP_VAR, UNDER_RATIO),\n      function(.x, .y, .under_ratio) {\n        if (.y == \"APR\") {\n          \n          recipe(\n            reformulate(\n              response = .y, \n              termlabels = .x %>% select(-all_of(.y)) %>% names()\n            ), \n            data = .x\n          ) %>%\n            step_nzv(all_predictors(), -all_outcomes()) %>%\n            step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%\n            step_impute_mode(all_nominal_predictors(), -all_outcomes()) %>%\n            step_dummy(all_nominal_predictors(), -all_outcomes()) %>% \n            themis::step_downsample(all_of(.y), under_ratio = .under_ratio) %>%\n            step_zv(all_predictors(), -all_outcomes()) %>% \n            step_center(all_predictors(), -all_outcomes()) %>% \n            step_scale(all_predictors(), -all_outcomes())\n          \n        } else {\n          \n          recipe(\n            reformulate(\n              response = .y, \n              termlabels = .x %>% select(-all_of(.y)) %>% names()\n            ), \n            data = .x\n          ) %>%\n            step_nzv(all_predictors(), -all_outcomes()) %>%\n            step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%\n            step_impute_mode(all_nominal_predictors(), -all_outcomes()) %>%\n            step_dummy(all_nominal_predictors(), -all_outcomes()) %>%\n            step_center(all_numeric_predictors(), -all_outcomes()) %>%\n            step_scale(all_numeric_predictors(), -all_outcomes())\n          \n        }\n      }\n    ),\n    \n    # * Set up workflow (done for each data table) ----\n    WORKFLOW = map(\n      RECIPE, ~workflows::workflow() %>% \n        workflows::add_model(logistic_model) %>% \n        workflows::add_recipe(.x)\n    ),\n    \n    # * Use the prep/bake functions on the training data ----\n    BAKED_TRAIN_DATA = map2(\n      data_train_tbl, RECIPE, ~ recipes::prep(\n        .y, training = .x\n      ) %>% \n        recipes::bake(new_data = NULL)\n    ),\n    \n    # * Use the prep/bake functions on the test data ----\n    BAKED_TEST_DATA = pmap(\n      list(data_train_tbl, RECIPE, data_test_tbl), function(.x, .y, .z) recipes::prep(\n        .y, training = .x\n      ) %>% \n        recipes::bake(new_data = .z)\n    ),\n    \n  )"
  },
  {
    "objectID": "air2025.html#model-refitting",
    "href": "air2025.html#model-refitting",
    "title": "Flexing Data: How Campus Gym Engagement Boosts Student Performance",
    "section": "Model Refitting",
    "text": "Model Refitting\nWith the optimized preprocessing workflows in place—including tuned downsampling ratios for the APR models—we proceeded to refit each model on the full training data. Using the same logistic regression specification as before, we applied the updated workflows to generate new final model fits. A fixed seed was maintained for reproducibility.\nEach dependent variable was cast as a factor to ensure proper handling by the classification model, and model training was conducted independently for each scenario. After fitting, we generated predictions on the respective test datasets. As before, each prediction output was paired with the true class label, forming the basis for final performance evaluation.\nThis step finalized the modeling pipeline with refined inputs and optimal preprocessing, ensuring that the models reflect the best-performing configurations identified during tuning while maintaining consistency in workflow structure and reproducibility.\n\n\nShow Code\nstudent_data_model_refit_tbl <- student_data_model_refit_tbl %>% \n  \n  mutate(\n    \n    # * Fit the model based on specifications ----\n    FIT_MODEL = pmap(\n      list(DEP_VAR, WORKFLOW, data_train_tbl, SEED), \n      function(.DEP_VAR, .WORKFLOW, .data_train_tbl, .seed) {\n        set.seed(.seed)\n        parsnip::fit(.WORKFLOW, data = .data_train_tbl %>% mutate(across(all_of(.DEP_VAR), as.factor)))\n      }\n    )\n  )\n\nstudent_data_model_refit_tbl <- student_data_model_refit_tbl %>% \n  \n  mutate(\n    \n    # * Get model predictions ----\n    PREDICT_MODEL = pmap(\n      list(FIT_MODEL, data_test_tbl, DEP_VAR),\n      function(.FIT_MODEL, .test_data, .DEP_VAR) {\n        .test_data %>% \n          select(all_of(.DEP_VAR)) %>% \n          bind_cols(\n            predict(.FIT_MODEL, new_data = .test_data)\n          )\n      }\n    )\n    \n  )"
  }
]